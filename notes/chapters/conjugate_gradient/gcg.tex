\subsection{General Conjugate Gradient}
Now that we are able to solve for the minimum eigenvalue of a matrix, we would like to extend our analysis to the smallest \textbf{nev} eigenpairs.
\\A minimization of the Rayleigh quotient could be used in principle, but searching for a solution $x$ which is a matrix $n\times\text{nev}$ needs to be done ensuring orthogonality of the columns.
\\This can be numerically expensive and unstable.
\\An alternative, efficient solution is to use the generalized conjugate gradient method.
\\It's a subspace projection method, where the search directions are calculated at each iteration by performing a Rayleigh Ritz procedure on the orthogonalized block $V=[X, P, W]$.

\begin{algorithm}[H]
\caption{GCG Algorithm}
\begin{algorithmic}[1]
\State \textbf{Input:} Matrices \( A \), \( B \), number of desired eigenpairs \( \texttt{nev} \)
\State Initialize block \( X \) with \( \texttt{nev} \) orthonormal vectors
\State Initialize blocks \( P \) and \( W \) with \( \texttt{nev} \) null vectors
\State Solve the Rayleigh Ritz problem \(X^T A X C= X^T B X C \Lambda \) with \( \texttt{nev} \) eigenpairs
\State Update \(X=X C\)
\While{not converged}
    \State Solve approximately \( AW = BX\Lambda \) with some CGM steps
    \State B-Orthogonalize \( V=[X, P, W] \)
    \State Solve the Rayleigh Ritz problem \(V^T A V C= C\Lambda \)
    \State Update \(X_\text{new} =V C\)
    \State Compute the residual \( R = AX_\text{new} - B\Lambda X \) 
    \State If \( \norm{R} < \epsilon \) then converged
    \State Otherwise, compute \( P = X_\text{new} \setminus X \) 
\EndWhile
\State \textbf{Output:} Approximate eigenpairs \( (\Lambda, X) \)
\end{algorithmic}
\end{algorithm}
Given the search subspace $X$, the Rayleigh-Ritz procedure gives us the best approximation $\Lambda, C$ to the eigenpair of the large scale problem.
\\Thus, we need a larger basis to explore better solutions, which comes in the form of $W$ and $P$
\\The block $\mathbf W$ is calculated from the inverse power method: Applying $A^{-1}$ to a vector we get an enhancement of the correct components.
\\The block $\mathbf P$ is calculated from the last search direction. This ensures after orthonormalization of the $V$ matrix that the new approximation $X$ is orthogonal to it, as in the simple CGM.
\\This procedure enhances stability and convergence speed by somewhat preventing the exploration of previously investigated subspaces.

%\section*{Modified Block Orthogonalization (Algorithm 3)}

%Given a matrix \( V \) with columns \( V = [V_1, V_2] \), where \( V_1 \) is already orthogonal, orthogonalize \( V_2 \) against \( V_1 \) and then within itself.

%\begin{algorithm}[H]
%\caption{Modified Block Orthogonalization}
%\begin{algorithmic}[1]
%\State \textbf{Input:} Matrix \( V = [V_1, V_2] \)
%\State \( H = V_1^T V_2 \)
%\State \( V_2 = V_2 - V_1 H \)
%\State Reorthogonalize \( V_2 \) (optional, based on reorth\_count)
%\State QR factorization: \( V_2 = QR \)
%\State \textbf{Output:} \( [V_1, Q] \) orthogonal matrix
%\end{algorithmic}
%\end{algorithm}

%\section*{Rayleigh-Ritz Procedure}

%Let \( V \) be an orthonormal basis of subspace. The Rayleigh-Ritz procedure is as follows:

%\begin{enumerate}
%\item Form the projected matrix:
%\[
%\bar{A} = V^T A V
%\]
%\item Solve the eigenvalue problem:
%\[
%\bar{A}C = C\Lambda
%\]
%\item Update the approximate eigenvectors:
%\[
%X = V C
%\]
%\end{enumerate}